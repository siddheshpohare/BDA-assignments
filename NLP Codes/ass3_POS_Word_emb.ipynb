{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKfmRev4CQ6U",
        "outputId": "d5b91524-fbda-4733-eb47-cce25878f3a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading spaCy model...\n",
            "NLP Analysis Tool: POS Tagging and Word Embeddings\n",
            "\n",
            "This program analyzes text using spaCy to perform:\n",
            "1. Part-of-Speech tagging\n",
            "2. Word embedding generation\n",
            "3. Similar word finding based on vector similarity\n",
            "\n",
            "\n",
            "===== TEXT ANALYSIS =====\n",
            "Input text: \"The quick brown fox jumps over the lazy dog.\"\n",
            "\n",
            "=== PART-OF-SPEECH TAGGING ===\n",
            "The: DET\n",
            "quick: ADJ\n",
            "brown: ADJ\n",
            "fox: NOUN\n",
            "jumps: VERB\n",
            "over: ADP\n",
            "the: DET\n",
            "lazy: ADJ\n",
            "dog: NOUN\n",
            ".: PUNCT\n",
            "\n",
            "=== WORD EMBEDDINGS ===\n",
            "Generated embeddings for 10 words\n",
            "\n",
            "Embedding for 'The' (showing first 5 dimensions):\n",
            "[-0.65276  0.23873 -0.23325  0.18608  0.37674]\n",
            "\n",
            "Words similar to 'The':\n",
            "  the: 1.0000\n",
            "  .: 0.3697\n",
            "  quick: 0.3498\n",
            "  over: 0.3326\n",
            "  lazy: 0.1891\n",
            "\n",
            "Embedding for 'quick' (showing first 5 dimensions):\n",
            "[-0.60053   0.18838  -0.40993   0.3225    0.070322]\n",
            "\n",
            "Words similar to 'quick':\n",
            "  .: 0.5222\n",
            "  lazy: 0.4098\n",
            "  The: 0.3498\n",
            "  the: 0.3498\n",
            "  over: 0.3182\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "===== TEXT ANALYSIS =====\n",
            "Input text: \"Natural language processing helps computers understand human language.\"\n",
            "\n",
            "=== PART-OF-SPEECH TAGGING ===\n",
            "Natural: ADJ\n",
            "language: NOUN\n",
            "processing: NOUN\n",
            "helps: VERB\n",
            "computers: NOUN\n",
            "understand: VERB\n",
            "human: ADJ\n",
            "language: NOUN\n",
            ".: PUNCT\n",
            "\n",
            "=== WORD EMBEDDINGS ===\n",
            "Generated embeddings for 8 words\n",
            "\n",
            "Embedding for 'Natural' (showing first 5 dimensions):\n",
            "[-0.66059   0.2348   -0.021227 -0.32737  -0.062493]\n",
            "\n",
            "Words similar to 'Natural':\n",
            "  human: 0.4209\n",
            "  computers: 0.3626\n",
            "  helps: 0.3582\n",
            "  processing: 0.2888\n",
            "  .: 0.2595\n",
            "\n",
            "Embedding for 'language' (showing first 5 dimensions):\n",
            "[-0.62498 -0.8816  -0.60641  0.33662  0.23677]\n",
            "\n",
            "Words similar to 'language':\n",
            "  understand: 0.2633\n",
            "  processing: 0.2222\n",
            "  human: 0.2154\n",
            "  .: 0.2035\n",
            "  computers: 0.1802\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "===== TEXT ANALYSIS =====\n",
            "Input text: \"Machine learning models require large amounts of training data.\"\n",
            "\n",
            "=== PART-OF-SPEECH TAGGING ===\n",
            "Machine: NOUN\n",
            "learning: NOUN\n",
            "models: NOUN\n",
            "require: VERB\n",
            "large: ADJ\n",
            "amounts: NOUN\n",
            "of: ADP\n",
            "training: NOUN\n",
            "data: NOUN\n",
            ".: PUNCT\n",
            "\n",
            "=== WORD EMBEDDINGS ===\n",
            "Generated embeddings for 10 words\n",
            "\n",
            "Embedding for 'Machine' (showing first 5 dimensions):\n",
            "[-0.72883    0.20718   -0.0033379 -0.0027673 -0.17204  ]\n",
            "\n",
            "Words similar to 'Machine':\n",
            "  models: 0.5523\n",
            "  data: 0.4077\n",
            "  learning: 0.4056\n",
            "  training: 0.4056\n",
            "  require: 0.3461\n",
            "\n",
            "Embedding for 'learning' (showing first 5 dimensions):\n",
            "[-0.9261   0.36204 -0.15093 -0.37449 -0.42103]\n",
            "\n",
            "Words similar to 'learning':\n",
            "  training: 1.0000\n",
            "  models: 0.4744\n",
            "  data: 0.4159\n",
            "  Machine: 0.4056\n",
            "  require: 0.2996\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Enter your own text to analyze (or type 'exit' to quit):\n",
            "> Play the game and enjoy\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the English model from spaCy\n",
        "print(\"Loading spaCy model...\")\n",
        "nlp = spacy.load('en_core_web_md')  # Medium model with word vectors\n",
        "\n",
        "def perform_pos_tagging(text):\n",
        "    \"\"\"\n",
        "    Perform Part-of-Speech tagging on input text\n",
        "    Returns a list of (word, POS tag) tuples\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    tagged_words = [(token.text, token.pos_) for token in doc]\n",
        "    return tagged_words\n",
        "\n",
        "def get_word_embeddings(text):\n",
        "    \"\"\"\n",
        "    Generate word embeddings for each token in the input text\n",
        "    Returns a dictionary mapping words to their vector representations\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    word_embeddings = {}\n",
        "\n",
        "    for token in doc:\n",
        "        if token.has_vector:\n",
        "            word_embeddings[token.text] = token.vector\n",
        "\n",
        "    return word_embeddings\n",
        "\n",
        "def find_similar_words(word, embeddings_dict, top_n=5):\n",
        "    \"\"\"\n",
        "    Find words most similar to the input word based on cosine similarity\n",
        "    Returns a list of (word, similarity score) tuples\n",
        "    \"\"\"\n",
        "    if word not in embeddings_dict:\n",
        "        return []\n",
        "\n",
        "    target_vector = embeddings_dict[word]\n",
        "    similarities = {}\n",
        "\n",
        "    for other_word, vector in embeddings_dict.items():\n",
        "        if other_word != word:\n",
        "            similarity = cosine_similarity([target_vector], [vector])[0][0]\n",
        "            similarities[other_word] = similarity\n",
        "\n",
        "    # Sort by similarity score (descending)\n",
        "    similar_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "    return similar_words[:top_n]\n",
        "\n",
        "def analyze_text(text):\n",
        "    \"\"\"\n",
        "    Perform comprehensive NLP analysis on input text\n",
        "    \"\"\"\n",
        "    print(\"\\n===== TEXT ANALYSIS =====\")\n",
        "    print(f\"Input text: \\\"{text}\\\"\")\n",
        "\n",
        "    # POS Tagging\n",
        "    print(\"\\n=== PART-OF-SPEECH TAGGING ===\")\n",
        "    tagged_words = perform_pos_tagging(text)\n",
        "    for word, pos in tagged_words:\n",
        "        print(f\"{word}: {pos}\")\n",
        "\n",
        "    # Word Embeddings\n",
        "    print(\"\\n=== WORD EMBEDDINGS ===\")\n",
        "    embeddings = get_word_embeddings(text)\n",
        "    print(f\"Generated embeddings for {len(embeddings)} words\")\n",
        "\n",
        "    # Sample a few words to show their embedding dimensions\n",
        "    sample_words = list(embeddings.keys())[:2]\n",
        "    for word in sample_words:\n",
        "        vector = embeddings[word]\n",
        "        print(f\"\\nEmbedding for '{word}' (showing first 5 dimensions):\")\n",
        "        print(vector[:5])\n",
        "\n",
        "        # Find similar words\n",
        "        print(f\"\\nWords similar to '{word}':\")\n",
        "        similar = find_similar_words(word, embeddings)\n",
        "        for similar_word, score in similar:\n",
        "            print(f\"  {similar_word}: {score:.4f}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example sentences to analyze\n",
        "    examples = [\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"Natural language processing helps computers understand human language.\",\n",
        "        \"Machine learning models require large amounts of training data.\"\n",
        "    ]\n",
        "\n",
        "    print(\"NLP Analysis Tool: POS Tagging and Word Embeddings\\n\")\n",
        "    print(\"This program analyzes text using spaCy to perform:\")\n",
        "    print(\"1. Part-of-Speech tagging\")\n",
        "    print(\"2. Word embedding generation\")\n",
        "    print(\"3. Similar word finding based on vector similarity\\n\")\n",
        "\n",
        "    # Analyze each example\n",
        "    for example in examples:\n",
        "        analyze_text(example)\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "\n",
        "    # Interactive mode\n",
        "    print(\"\\nEnter your own text to analyze (or type 'exit' to quit):\")\n",
        "    user_text = input(\"> \")\n",
        "    if user_text.lower() == 'exit':\n",
        "        analyze_text(user_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHMO6OCwDMt_",
        "outputId": "d1a02248-8256-430e-9544-d80b4bf6a8dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')  # Correct resource name (no \"_eng\" suffix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFKFAKTHD0Jp",
        "outputId": "c89e1025-47e4-481f-af5f-577432592783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_g6Jm1dJiG5"
      },
      "outputs": [],
      "source": [
        "'''âœ… What the Code Does (Summary)\n",
        "This Python script is an NLP analysis tool built with spaCy. It performs:\n",
        "\n",
        "Part-of-Speech (POS) tagging\n",
        "\n",
        "Word embedding generation\n",
        "\n",
        "Cosine similarity-based similar word detection\n",
        "\n",
        "ğŸ“š Dataset Used\n",
        "No external dataset is used.\n",
        "\n",
        "It analyzes manually input or hardcoded text samples.\n",
        "\n",
        "ğŸ“¦ Libraries Used\n",
        "spaCy: For NLP operations (POS tagging and word embeddings)\n",
        "\n",
        "numpy: For numerical vector manipulation\n",
        "\n",
        "sklearn.metrics.pairwise.cosine_similarity: For finding similar words via vector comparison\n",
        "\n",
        "(Optional/incorrectly placed): nltk.download() is at the end, not relevant for this code since POS tagging is handled by spaCy, not NLTK.\n",
        "\n",
        "ğŸ§  Possible Viva Questions and Answers\n",
        "ğŸ”¹ Section 1: POS Tagging\n",
        "â“ Q1. What is POS tagging?\n",
        "A:\n",
        "Part-of-Speech tagging assigns each word in a sentence its grammatical role, such as noun, verb, adjective, etc.\n",
        "\n",
        "â“ Q2. How is POS tagging performed in this code?\n",
        "A:\n",
        "Using spaCyâ€™s nlp() function, which processes text into a Doc object. We then extract the POS tags using token.pos_.\n",
        "\n",
        "ğŸ”¹ Section 2: Word Embeddings\n",
        "â“ Q3. What are word embeddings?\n",
        "A:\n",
        "They are dense vector representations of words in a high-dimensional space. Words with similar meanings are close together in this space.\n",
        "\n",
        "â“ Q4. What model does the code use for embeddings?\n",
        "A:\n",
        "It loads en_core_web_md, a medium-sized spaCy model that includes pretrained word vectors for English.\n",
        "\n",
        "â“ Q5. How are embeddings retrieved in the code?\n",
        "A:\n",
        "For each token in the text, token.vector gives the embedding if available (token.has_vector ensures this).\n",
        "\n",
        "ğŸ”¹ Section 3: Cosine Similarity\n",
        "â“ Q6. Why is cosine similarity used?\n",
        "A:\n",
        "To measure similarity between word vectors. A higher cosine value (close to 1) means the words are more semantically similar.\n",
        "\n",
        "â“ Q7. How is cosine similarity calculated?\n",
        "A:\n",
        "Using cosine_similarity() from sklearn. It compares the vector of the target word with other vectors in the dictionary.\n",
        "\n",
        "ğŸ”¹ Section 4: Design and Execution\n",
        "â“ Q8. What is the structure of the program?\n",
        "A:\n",
        "\n",
        "The main function analyze_text() performs all tasks on input text.\n",
        "\n",
        "It is called for both predefined examples and interactive user input.\n",
        "\n",
        "â“ Q9. What's wrong with the NLTK lines at the end?\n",
        "A:\n",
        "They are unnecessary and incorrectly placed. This script uses spaCy for tagging, not NLTK. The downloads for punkt_tab and averaged_perceptron_tagger are not relevant here.\n",
        "\n",
        "â“ Q10. What are some limitations of this code?\n",
        "A:\n",
        "\n",
        "It only compares words within the input sentence, not with a larger vocabulary.\n",
        "\n",
        "It doesnâ€™t use context-aware embeddings like BERT.\n",
        "\n",
        "It doesnâ€™t preprocess text (e.g., stopword removal, lemmatization).'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
