{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67cf3iFIuaYj",
        "outputId": "d8f288d3-4359-4e3e-9ec2-624a50f5a041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            headline Language  Word Count  \\\n",
            "0              143 Miles in 35 Days: Lessons Learned       fr           8   \n",
            "1       Talking to Yourself: Crazy or Crazy Helpful?       en           9   \n",
            "2  Crenezumab: Trial Will Gauge Whether Alzheimer...       en          15   \n",
            "3                     Oh, What a Difference She Made       en           7   \n",
            "4                                   Green Superfoods       af           2   \n",
            "\n",
            "   Sentence Count                                             Tokens  \n",
            "0               1    [143, Miles, in, 35, Days, :, Lessons, Learned]  \n",
            "1               1  [Talking, to, Yourself, :, Crazy, or, Crazy, H...  \n",
            "2               1  [Crenezumab, :, Trial, Will, Gauge, Whether, A...  \n",
            "3               1            [Oh, ,, What, a, Difference, She, Made]  \n",
            "4               1                                [Green, Superfoods]  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import langdetect\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return langdetect.detect(text)\n",
        "    except:\n",
        "        return \"Unknown\"\n",
        "\n",
        "def word_count(text):\n",
        "    return len(word_tokenize(text))\n",
        "\n",
        "def sentence_count(text):\n",
        "    return len(sent_tokenize(text))\n",
        "\n",
        "def process_headlines(dataset_path):\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    df['Language'] = df['headline'].apply(detect_language)\n",
        "    df['Word Count'] = df['headline'].apply(word_count)\n",
        "    df['Sentence Count'] = df['headline'].apply(sentence_count)\n",
        "    df['Tokens'] = df['headline'].apply(word_tokenize)\n",
        "    print(df[['headline', 'Language', 'Word Count', 'Sentence Count', 'Tokens']].head())\n",
        "\n",
        "dataset_path = \"NewsCategorizer.csv\"\n",
        "process_headlines(dataset_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 1. Importing Required Libraries\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "    Why: pandas is used for loading and manipulating structured data (like CSV).\n",
        "\n",
        "    Use: We’ll use it to read the dataset and process each row (headline).\n",
        "\n",
        "import langdetect\n",
        "\n",
        "    Why: langdetect is a library used to detect the language of a given text.\n",
        "\n",
        "    Use: We'll use this to identify the language of each headline.\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "    Why: nltk is a popular library for Natural Language Processing (NLP).\n",
        "\n",
        "        word_tokenize: splits text into words.\n",
        "\n",
        "        sent_tokenize: splits text into sentences.\n",
        "\n",
        "    Use: These are used to count words, count sentences, and tokenize text.\n",
        "\n",
        "🔹 2. Download Required Tokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "    Why: Downloads the Punkt tokenizer model, which is needed by NLTK to tokenize words and sentences.\n",
        "\n",
        "    Use: Without this, word_tokenize and sent_tokenize won’t work.\n",
        "\n",
        "🔹 3. Define Helper Functions\n",
        "📌 Language Detection\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return langdetect.detect(text)\n",
        "    except:\n",
        "        return \"Unknown\"\n",
        "\n",
        "    What it does: Tries to detect the language of the input text.\n",
        "\n",
        "    try-except block: Prevents the program from crashing if detection fails.\n",
        "\n",
        "    Returns: Language code (like \"en\" for English), or \"Unknown\" if detection fails.\n",
        "\n",
        "📌 Word Count\n",
        "\n",
        "def word_count(text):\n",
        "    return len(word_tokenize(text))\n",
        "\n",
        "    What it does: Tokenizes the text into words and returns the number of tokens.\n",
        "\n",
        "    Use: This gives the number of words in the text.\n",
        "\n",
        "📌 Sentence Count\n",
        "\n",
        "def sentence_count(text):\n",
        "    return len(sent_tokenize(text))\n",
        "\n",
        "    What it does: Tokenizes the text into sentences and returns the count.\n",
        "\n",
        "    Use: Gives the number of sentences in the headline.\n",
        "\n",
        "🔹 4. Process Dataset Function\n",
        "\n",
        "def process_headlines(dataset_path):\n",
        "\n",
        "    Defines a function to load the dataset and apply all processing steps.\n",
        "\n",
        "📌 Load Dataset\n",
        "\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    What it does: Reads the CSV file into a DataFrame df.\n",
        "\n",
        "    Use: The dataset is assumed to have a column named \"headline\".\n",
        "\n",
        "📌 Apply NLP Functions to Each Headline\n",
        "\n",
        "    df['Language'] = df['headline'].apply(detect_language)\n",
        "\n",
        "    Detects language for each headline.\n",
        "\n",
        "    Adds a new column Language to store the result.\n",
        "\n",
        "    df['Word Count'] = df['headline'].apply(word_count)\n",
        "\n",
        "    Counts words in each headline.\n",
        "\n",
        "    Adds the count to a new column Word Count.\n",
        "\n",
        "    df['Sentence Count'] = df['headline'].apply(sentence_count)\n",
        "\n",
        "    Counts sentences in each headline.\n",
        "\n",
        "    Saves result in the Sentence Count column.\n",
        "\n",
        "    df['Tokens'] = df['headline'].apply(word_tokenize)\n",
        "\n",
        "    Tokenizes each headline into words.\n",
        "\n",
        "    Adds the list of words into a new column Tokens.\n",
        "\n",
        "📌 Display Result\n",
        "\n",
        "    print(df[['headline', 'Language', 'Word Count', 'Sentence Count', 'Tokens']].head())\n",
        "\n",
        "    Displays the first 5 rows of selected columns.\n",
        "\n",
        "    Good for verification that all processing worked correctly.\n",
        "\n",
        "🔹 5. Set Dataset Path & Run the Function\n",
        "\n",
        "dataset_path = \"NewsCategorizer.csv\"\n",
        "\n",
        "    Sets the path to your dataset CSV file (make sure it's in the same directory or provide full path).\n",
        "\n",
        "process_headlines(dataset_path)\n",
        "\n",
        "    Calls the function to run all analysis on the dataset.\n",
        "\n",
        "✅ Summary for Viva:\n",
        "\n",
        "    This script processes headlines from a CSV file.\n",
        "\n",
        "    It applies language detection, word count, sentence count, and tokenization using langdetect and nltk.\n",
        "\n",
        "    Results are stored as new columns in the DataFrame.\n",
        "\n",
        "    Helps in preprocessing text for NLP tasks like classification, summarization, or clustering."
      ],
      "metadata": {
        "id": "MOgkOBjt1Zo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ne8Y6t0vQTW",
        "outputId": "25adf2c5-8669-4672-d1ee-575a24f1d65a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=a44d0078792344fa4e9c085851f1ca7a174e621d79205974af053da17869c8f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    }
  ]
}