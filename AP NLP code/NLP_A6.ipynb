{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkwQj0C35lCw",
        "outputId": "c4294321-0272-4e4f-b37a-1b08b3081183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of the sequence 'the n-gram': 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import bigrams\n",
        "from nltk.probability import FreqDist, ConditionalFreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Sample text (you can replace this with any corpus or large text)\n",
        "text = \"This is a sample sentence for N-gram model. The N-gram model predicts probabilities.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text.lower())  # Convert to lowercase to standardize\n",
        "\n",
        "# Generate bigrams (you can change this to ngrams for other values of N)\n",
        "bigrams_list = list(bigrams(tokens))\n",
        "\n",
        "# Create frequency distribution for unigrams and bigrams\n",
        "unigram_freq = FreqDist(tokens)\n",
        "bigram_freq = FreqDist(bigrams_list)\n",
        "\n",
        "# Create a conditional frequency distribution for bigrams\n",
        "cfdist = ConditionalFreqDist(bigrams_list)\n",
        "\n",
        "# Calculate the probability of a given sequence of words\n",
        "def calculate_bigram_probability(sequence):\n",
        "    sequence_tokens = word_tokenize(sequence.lower())  # Tokenize the sequence\n",
        "    probability = 1.0\n",
        "\n",
        "    for i in range(1, len(sequence_tokens)):\n",
        "        previous_word = sequence_tokens[i - 1]\n",
        "        current_word = sequence_tokens[i]\n",
        "\n",
        "        # Calculate the conditional probability P(current_word | previous_word)\n",
        "        count_previous_word = unigram_freq[previous_word]\n",
        "        count_bigram = bigram_freq[(previous_word, current_word)]\n",
        "\n",
        "        # If the previous word is not in the unigram distribution, the probability is 0\n",
        "        if count_previous_word == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Compute the probability for the bigram\n",
        "        probability *= count_bigram / count_previous_word\n",
        "\n",
        "    return probability\n",
        "\n",
        "# Test the function with a sequence of words\n",
        "sequence = \"the n-gram\"\n",
        "probability = calculate_bigram_probability(sequence)\n",
        "\n",
        "print(f\"Probability of the sequence '{sequence}': {probability}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the probability of a sequence of words using an N-gram language model, we need to follow these steps:\n",
        "\n",
        "    Tokenization: Break the text into words.\n",
        "\n",
        "    N-gram Construction: Construct N-grams from the tokens. An N-gram is a contiguous sequence of N words.\n",
        "\n",
        "    Count N-grams: Count the occurrences of each N-gram in the training corpus.\n",
        "\n",
        "    Calculate Probability: The probability of a sequence of words is computed using the formula:\n",
        "\n",
        "P(w1,w2,...,wn)=‚àèi=1nP(wi‚à£w1,...,wi‚àí1)\n",
        "P(w1‚Äã,w2‚Äã,...,wn‚Äã)=i=1‚àèn‚ÄãP(wi‚Äã‚à£w1‚Äã,...,wi‚àí1‚Äã)\n",
        "\n",
        "For a bigram model (N=2), the probability of a sequence of words would be:\n",
        "P(w1,w2)=P(w1)‚ãÖP(w2‚à£w1)\n",
        "P(w1‚Äã,w2‚Äã)=P(w1‚Äã)‚ãÖP(w2‚Äã‚à£w1‚Äã)\n",
        "\n",
        "In a more generalized N-gram model:\n",
        "P(w1,w2,...,wn)=‚àèi=2nP(wi‚à£wi‚àí1)\n",
        "P(w1‚Äã,w2‚Äã,...,wn‚Äã)=i=2‚àèn‚ÄãP(wi‚Äã‚à£wi‚àí1‚Äã)\n",
        "\n",
        "Where P(w_i | w_{i-1}) is the conditional probability of the current word given the previous one, computed as:\n",
        "P(wi‚à£wi‚àí1)=Count(wi‚àí1,wi)Count(wi‚àí1)\n",
        "P(wi‚Äã‚à£wi‚àí1‚Äã)=Count(wi‚àí1‚Äã)Count(wi‚àí1‚Äã,wi‚Äã)‚Äã"
      ],
      "metadata": {
        "id": "aDyhjHLG5-bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Breakdown of the Code:\n",
        "\n",
        "    nltk.download('punkt'): Downloads the tokenizer needed to split text into words.\n",
        "\n",
        "    word_tokenize(text.lower()): Tokenizes the input text and converts it to lowercase to standardize the words.\n",
        "\n",
        "    bigrams(tokens): This generates bigrams (sequences of 2 words). You can replace bigrams with ngrams for larger N-grams.\n",
        "\n",
        "    FreqDist and ConditionalFreqDist:\n",
        "\n",
        "        FreqDist counts the frequency of unigrams and bigrams.\n",
        "\n",
        "        ConditionalFreqDist stores bigram frequencies conditioned on the first word (useful for bigram models).\n",
        "\n",
        "    calculate_bigram_probability(sequence):\n",
        "\n",
        "        Tokenizes the input sequence.\n",
        "\n",
        "        Computes the conditional probability of each word given its previous word.\n",
        "\n",
        "        Multiplies the probabilities for each word in the sequence to get the overall probability.\n",
        "\n",
        "üß† Example Output:\n",
        "\n",
        "If you test with the sequence \"the n-gram\" from the sample text, the output will be something like:\n",
        "\n",
        "Probability of the sequence 'the n-gram': 0.025\n",
        "\n",
        "The result shows the probability of the bigram sequence \"the n-gram\" based on the frequency of the bigrams in the corpus.\n",
        "üí° Notes:\n",
        "\n",
        "    N-gram size: The above example uses bigrams (N=2). You can extend it to trigrams (N=3), 4-grams, etc., by simply changing the n-gram generation from bigrams to ngrams and adjusting the model accordingly.\n",
        "\n",
        "    Smoothing: In real applications, smoothing techniques like Laplace smoothing are used to handle zero probabilities (e.g., when a particular bigram or trigram doesn't exist in the training corpus).\n",
        "\n",
        "    Corpus Size: The program uses a small sample text. For better results, you should train the model on a larger corpus to get more accurate probabilities.\n",
        "\n",
        "üéì Tip for Viva:\n",
        "\n",
        "If asked about N-grams, you can say:\n",
        "\n",
        "    \"N-gram models estimate the probability of a word based on the previous N-1 words. For example, a bigram model predicts the probability of a word given the previous word, and a trigram model predicts based on the previous two words.\""
      ],
      "metadata": {
        "id": "SDy3nQ1G6Ode"
      }
    }
  ]
}