{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrS_R6MDE2_O",
        "outputId": "ab22932c-5d12-4c48-9efa-cb31bcd11ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C[0][0] = 58\n",
            "C[1][1] = 154\n",
            "C[0][1] = 64\n",
            "C[1][0] = 139\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setAppName(\"MatrixMultiplication\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Sample matrix A (2x3)\n",
        "A = [\n",
        "    (0, 0, 1), (0, 1, 2), (0, 2, 3),\n",
        "    (1, 0, 4), (1, 1, 5), (1, 2, 6)\n",
        "]\n",
        "\n",
        "# Sample matrix B (3x2)\n",
        "B = [\n",
        "    (0, 0, 7), (0, 1, 8),\n",
        "    (1, 0, 9), (1, 1, 10),\n",
        "    (2, 0, 11), (2, 1, 12)\n",
        "]\n",
        "\n",
        "# Convert to RDDs\n",
        "rdd_A = sc.parallelize(A)  # (i, k, A_ik)\n",
        "rdd_B = sc.parallelize(B)  # (k, j, B_kj)\n",
        "\n",
        "# Map step: emit intermediate keys with tags\n",
        "mapped_A = rdd_A.flatMap(lambda x: [((x[0], j), ('A', x[1], x[2])) for j in range(2)])  # 2 is number of columns in B\n",
        "mapped_B = rdd_B.flatMap(lambda x: [((i, x[1]), ('B', x[0], x[2])) for i in range(2)])  # 2 is number of rows in A\n",
        "\n",
        "# Combine and group by key\n",
        "joined = mapped_A.union(mapped_B).groupByKey()\n",
        "\n",
        "# Reduce step: multiply matching entries and sum\n",
        "def multiply(values):\n",
        "    A_vals = {k: v for tag, k, v in values if tag == 'A'}\n",
        "    B_vals = {k: v for tag, k, v in values if tag == 'B'}\n",
        "    total = sum(A_vals.get(k, 0) * B_vals.get(k, 0) for k in set(A_vals) & set(B_vals))\n",
        "    return total\n",
        "\n",
        "result = joined.mapValues(multiply)\n",
        "\n",
        "# Display result as (i, j, C_ij)\n",
        "for ((i, j), val) in result.collect():\n",
        "    print(f\"C[{i}][{j}] = {val}\")\n",
        "\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are multiplying two matrices using a MapReduce paradigm in PySpark, which simulates Hadoop-style processing using big data functions like map, flatMap, groupByKey, and reduce.\n",
        "ðŸ”¢ Example\n",
        "\n",
        "Given:\n",
        "\n",
        "Matrix A (2x3):\n",
        "\n",
        "A = [[1, 2, 3],\n",
        "     [4, 5, 6]]\n",
        "\n",
        "Matrix B (3x2):\n",
        "\n",
        "B = [[7, 8],\n",
        "     [9, 10],\n",
        "     [11, 12]]\n",
        "\n",
        "We want to compute Matrix C = A Ã— B, which will be a 2x2 matrix.\n",
        "ðŸ’¡ Step-by-Step Explanation\n",
        "âœ… 1. RDD Initialization\n",
        "\n",
        "rdd_A = sc.parallelize(A)  # Format: (i, k, A[i][k])\n",
        "rdd_B = sc.parallelize(B)  # Format: (k, j, B[k][j])\n",
        "\n",
        "We load both matrices into Resilient Distributed Datasets (RDDs), which are the fundamental data structure in Spark for handling large-scale distributed data.\n",
        "âœ… 2. Map Phase (flatMap)\n",
        "\n",
        "mapped_A = rdd_A.flatMap(lambda x: [((x[0], j), ('A', x[1], x[2])) for j in range(2)])\n",
        "mapped_B = rdd_B.flatMap(lambda x: [((i, x[1]), ('B', x[0], x[2])) for i in range(2)])\n",
        "\n",
        "What This Does:\n",
        "\n",
        "    We're preparing each matrixâ€™s values so they can be joined by keys.\n",
        "\n",
        "    For every cell in A, we broadcast its value to all columns in B.\n",
        "\n",
        "    For every cell in B, we broadcast its value to all rows in A.\n",
        "\n",
        "    This step simulates the Map step in MapReduce.\n",
        "\n",
        "Big Data Concept:\n",
        "\n",
        "flatMap() is a transformation that allows parallel broadcasting of elements. It's distributed across all Spark workers.\n",
        "âœ… 3. Union & Grouping (shuffle phase)\n",
        "\n",
        "joined = mapped_A.union(mapped_B).groupByKey()\n",
        "\n",
        "    union() merges the two RDDs.\n",
        "\n",
        "    groupByKey() groups all emitted values for a particular cell (i, j) of result matrix C.\n",
        "\n",
        "    All the values needed to compute one cell C[i][j] are now together.\n",
        "\n",
        "Big Data Concept:\n",
        "\n",
        "This is the shuffle phase in Hadoop, where key-value pairs are grouped across the cluster.\n",
        "âœ… 4. Reduce Phase (multiplication)\n",
        "\n",
        "def multiply(values):\n",
        "    A_vals = {k: v for tag, k, v in values if tag == 'A'}\n",
        "    B_vals = {k: v for tag, k, v in values if tag == 'B'}\n",
        "    total = sum(A_vals.get(k, 0) * B_vals.get(k, 0) for k in set(A_vals) & set(B_vals))\n",
        "    return total\n",
        "\n",
        "result = joined.mapValues(multiply)\n",
        "\n",
        "    We now compute the dot product of the i-th row of A and j-th column of B.\n",
        "\n",
        "    Only matching indices (k) are multiplied and summed.\n",
        "\n",
        "Big Data Concept:\n",
        "\n",
        "    mapValues() is a transformation used to apply logic in parallel to all values of grouped keys â€” like the Reduce step.\n",
        "\n",
        "    This part benefits from Sparkâ€™s distributed in-memory computation.\n",
        "\n",
        "âœ… 5. Collecting the Result\n",
        "\n",
        "for ((i, j), val) in result.collect():\n",
        "    print(f\"C[{i}][{j}] = {val}\")\n",
        "\n",
        "    collect() brings results back from all nodes to the driver program.\n",
        "\n",
        "    Use only when final result is small (like our matrix).\n",
        "\n",
        "ðŸ§  Viva-Ready Big Data Concepts Used\n",
        "PySpark Function\tBig Data Equivalent\tPurpose\n",
        "parallelize()\tInput Splitting\tLoads data in parallel\n",
        "flatMap()\tMap Phase\tEmits intermediate key-value pairs\n",
        "union()\tMerge Mappers' Output\tCombines mapped datasets\n",
        "groupByKey()\tShuffle/Sort\tGroups values by key for reduction\n",
        "mapValues()\tReduce Phase\tComputes final values per key\n",
        "collect()\tOutput\tGathers result to single machine\n",
        "âœ… Why This is a Good Big Data Example\n",
        "\n",
        "    Works in parallel across distributed nodes.\n",
        "\n",
        "    Avoids loading entire matrices into memory at once.\n",
        "\n",
        "    Uses MapReduce paradigm naturally for a mathematical problem.\n",
        "\n",
        "    Scales to huge matrices with minor code change."
      ],
      "metadata": {
        "id": "4f_wOCXsF2Cz"
      }
    }
  ]
}