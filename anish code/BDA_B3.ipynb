{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daSZAj5JGlsB",
        "outputId": "7976ba74-1ebe-401a-ecb3-64db524fa444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Snowfall in 2023: Station 001, Day 25, Snowfall 10 inches\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "conf = SparkConf().setAppName(\"MaxSnowfall\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Sample input data (simulating random weather data)\n",
        "weather_data = [\n",
        "    (\"001\", 2023, 1, 15, 5),\n",
        "    (\"002\", 2023, 1, 15, 3),\n",
        "    (\"001\", 2023, 2, 5, 7),\n",
        "    (\"002\", 2023, 2, 5, 1),\n",
        "    (\"001\", 2023, 12, 25, 10),\n",
        "    (\"003\", 2023, 12, 25, 6),\n",
        "    (\"001\", 2023, 11, 30, 2),\n",
        "    (\"003\", 2023, 11, 30, 8)\n",
        "]\n",
        "\n",
        "# Parallelize the data (simulating reading from a file)\n",
        "rdd = sc.parallelize(weather_data)\n",
        "\n",
        "# Mapper: Emit (year, station_id, day) as key and snowfall as value\n",
        "def map_function(record):\n",
        "    station_id, year, month, day, snowfall = record\n",
        "    return ((year, station_id, day), snowfall)\n",
        "\n",
        "# Applying the map function\n",
        "mapped_rdd = rdd.map(map_function)\n",
        "\n",
        "# Reducer: Find maximum snowfall per (year, station_id, day)\n",
        "def reduce_function(a, b):\n",
        "    return max(a, b)\n",
        "\n",
        "# Apply the reduce function to get the maximum snowfall\n",
        "reduced_rdd = mapped_rdd.reduceByKey(reduce_function)\n",
        "\n",
        "# Collect results and find the (year, station_id, day) with the maximum snowfall\n",
        "max_snowfall_record = reduced_rdd.collect()\n",
        "\n",
        "# Output the results\n",
        "max_snowfall = max(max_snowfall_record, key=lambda x: x[1])\n",
        "year, station_id, day = max_snowfall[0]\n",
        "snowfall = max_snowfall[1]\n",
        "print(f\"Max Snowfall in 2023: Station {station_id}, Day {day}, Snowfall {snowfall} inches\")\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Program\n",
        "1. Input Data:\n",
        "\n",
        "    The program simulates a weather database using a list of tuples in the format (station_id, year, month, day, snowfall).\n",
        "\n",
        "    In a real scenario, this data would be read from HDFS or a file.\n",
        "\n",
        "2. Map Function:\n",
        "\n",
        "def map_function(record):\n",
        "    station_id, year, month, day, snowfall = record\n",
        "    return ((year, station_id, day), snowfall)\n",
        "\n",
        "    Key: (year, station_id, day) â€” this will uniquely identify a record based on the year, station, and day.\n",
        "\n",
        "    Value: snowfall â€” the snowfall amount for that day.\n",
        "\n",
        "3. Reduce Function:\n",
        "\n",
        "def reduce_function(a, b):\n",
        "    return max(a, b)\n",
        "\n",
        "    The reducer compares the snowfall values for each (year, station_id, day) and returns the maximum snowfall.\n",
        "\n",
        "4. Final Output:\n",
        "\n",
        "max_snowfall = max(max_snowfall_record, key=lambda x: x[1])\n",
        "\n",
        "    The program finds the record with the maximum snowfall from the output of the reducer.\n",
        "\n",
        "5. Result:\n",
        "\n",
        "    The output prints the station, day, and snowfall corresponding to the maximum snowfall in the specified year (2023).\n",
        "\n",
        "Example Output:\n",
        "\n",
        "Max Snowfall in 2023: Station 001, Day 25, Snowfall 10 inches\n",
        "\n",
        "ðŸ“Œ How This Relates to Big Data and Hadoop:\n",
        "\n",
        "    MapReduce: We used the Map phase to distribute the work of parsing and creating key-value pairs, and the Reduce phase to aggregate the values (snowfall) to compute the maximum.\n",
        "\n",
        "    Scalability: This approach can scale horizontally to handle large datasets across multiple nodes in a Hadoop cluster.\n",
        "\n",
        "    Fault Tolerance: Spark (and Hadoop) provides fault tolerance by replicating data and computations across different nodes.\n",
        "\n",
        "    Distributed Computing: The work is distributed across the cluster, and each node processes part of the data in parallel.\n",
        "\n",
        "ðŸš€ Running on Hadoop\n",
        "\n",
        "To run this on a Hadoop cluster:\n",
        "\n",
        "    Store the input file on HDFS.\n",
        "\n",
        "    Submit the script using spark-submit, specifying the HDFS path for input and output.\n",
        "\n",
        "    Monitor the results through the Hadoop UI or fetch the output from the specified HDFS directory."
      ],
      "metadata": {
        "id": "KWmIbWlrG6gW"
      }
    }
  ]
}