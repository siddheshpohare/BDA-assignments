{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWwml1ccHEuP",
        "outputId": "f18116ba-0752-400f-853f-034a31a72f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movie 1 has an average rating of 3.75\n",
            "Movie 2 has an average rating of 4.5\n",
            "Movie 3 has an average rating of 2.5\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Initialize Spark\n",
        "conf = SparkConf().setAppName(\"MovieAvgRating\").setMaster(\"local\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Sample data (simulating lines in a CSV file)\n",
        "ratings_data = [\n",
        "    \"1,101,4.0,881250949\",\n",
        "    \"1,102,3.5,891717742\",\n",
        "    \"2,101,5.0,881250949\",\n",
        "    \"2,103,4.0,891717742\",\n",
        "    \"3,104,2.0,891717742\",\n",
        "    \"3,105,3.0,891717742\"\n",
        "]\n",
        "\n",
        "# Parallelize dataset\n",
        "rdd = sc.parallelize(ratings_data)\n",
        "\n",
        "# Map step: Emit (movie_id, rating)\n",
        "def map_function(line):\n",
        "    fields = line.split(\",\")\n",
        "    movie_id = fields[0]\n",
        "    rating = float(fields[2])\n",
        "    return (movie_id, rating)\n",
        "\n",
        "mapped_rdd = rdd.map(map_function)\n",
        "\n",
        "# Reduce step: Compute average rating\n",
        "def avg_reduce(values):\n",
        "    total = sum(values)\n",
        "    count = len(values)\n",
        "    return round(total / count, 2)\n",
        "\n",
        "# Group ratings by movie\n",
        "grouped_rdd = mapped_rdd.groupByKey()\n",
        "\n",
        "# Apply average calculation\n",
        "avg_ratings_rdd = grouped_rdd.mapValues(lambda ratings: avg_reduce(list(ratings)))\n",
        "\n",
        "# Collect and print result\n",
        "for movie_id, avg_rating in avg_ratings_rdd.collect():\n",
        "    print(f\"Movie {movie_id} has an average rating of {avg_rating}\")\n",
        "\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viva-Style Explanation\n",
        "Phase\tFunction\tExplanation\n",
        "Map\tmap_function\tConverts each line into (movie_id, rating) pairs.\n",
        "Shuffle\tgroupByKey()\tGroups all ratings by movie_id.\n",
        "Reduce\tmapValues() with avg_reduce\tCalculates average rating from list of ratings for each movie.\n",
        "Output\tcollect()\tGathers final results for printing or saving.\n",
        "ðŸ§  Big Data Concepts Applied:\n",
        "\n",
        "    RDDs: Distributed datasets that support fault-tolerant, parallel operations.\n",
        "\n",
        "    MapReduce Pattern: Uses Map â†’ Shuffle â†’ Reduce stages.\n",
        "\n",
        "    Parallelism: Efficiently computes average ratings even with millions of rows.\n",
        "\n",
        "    Fault Tolerance: Spark automatically handles node failures.\n",
        "\n",
        "    Scalability: Runs efficiently across large clusters.\n",
        "\n",
        "ðŸš€ To Scale with Real Hadoop Cluster:\n",
        "\n",
        "    Put ratings file on HDFS (e.g. hdfs://input/movies.csv)\n",
        "\n",
        "    Replace rdd = sc.parallelize(...) with:\n",
        "\n",
        "rdd = sc.textFile(\"hdfs://input/movies.csv\")\n",
        "\n",
        "Run via:\n",
        "\n",
        "spark-submit movie_avg_rating.py"
      ],
      "metadata": {
        "id": "AoTaUUNNHcE-"
      }
    }
  ]
}